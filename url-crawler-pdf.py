r"""
This script crawls a website using Scrapy, fetches fully rendered HTML using Pyppeteer,
saves each visited page as a PDF with the source URL included, and combines all generated PDFs
into a single file in the order the pages were visited.

Key Features:
- Handles dynamically rendered content using Pyppeteer (`fetch_and_parse`).
- Combines PDFs in the order in which the pages were visited.
- Visits only pages reachable from the starting URL that are in its subdirectories.
- Skips links to non-text files (e.g., `.exe`, `.zip`, `.tar`, `.gz`).
- Allows skipping URLs based on user-specified regular expressions for filename and directory patterns.
- Outputs verbose logs for debugging when the `--verbose` flag is enabled.

Usage:
1. Install dependencies: pip install scrapy pyppeteer PyPDF2 nest-asyncio.
   (or use source ~/web-scrawler_env/bin/activate)
2. Run the script with the starting URL, output PDF name, and optionally a regex to skip:
   python script.py <start_url> <output_filename> [--verbose] [--embed_url] [--skip-regex "<regex>"].
3. PDFs are saved in the "crawled_pdfs" directory, and combined into the specified output file.

For ex, to use on ISP docs generated by doxygen:
$ python3 url-crawler-pdf.py https://software-dl.ti.com/jacinto7/esd/processor-sdk-rtos-jacinto7/10_00_00_05/exports/docs/imaging/docs/user_guide/index.html test.pdf --skip-regex ".*/(.*_8h.*|dir_[a-fA-F0-9]+)\.html$"

"""

import scrapy
from scrapy.crawler import CrawlerProcess
from pyppeteer import launch
from PyPDF2 import PdfMerger
import nest_asyncio
import os
import asyncio
import re
from urllib.parse import urljoin, urlparse, urldefrag

# Apply the nest_asyncio patch
nest_asyncio.apply()

async def fetch_rendered_html(url):
    """
    Fetch the fully rendered HTML of a URL using Pyppeteer.
    """
    browser = await launch(headless=True)
    page = await browser.newPage()
    await page.goto(url, waitUntil="networkidle2")
    content = await page.content()
    await browser.close()
    return content


class PDFSpider(scrapy.Spider):
    name = "pdf_spider"

    def __init__(self, start_url, output_filename, skip_regex=None, verbose=False, embed_url=False, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.start_urls = [start_url]
        self.allowed_domains = [self.get_domain(start_url)]
        self.allowed_path_prefix = self.get_path_prefix(start_url)
        self.skip_regex = re.compile(skip_regex) if skip_regex else None
        self.base_dir = "crawled_pdfs"
        self.output_filename = output_filename
        self.verbose = verbose
        self.embed_url = embed_url  # New attribute to control URL embedding
        self.visited_urls = []  # Maintain the order of visited URLs
        self.visited_urls_set = set()  # To avoid duplicates when revisiting URLs

        if not os.path.exists(self.base_dir):
            os.makedirs(self.base_dir)

    def get_domain(self, url):
        parsed_url = urlparse(url)
        return parsed_url.netloc

    def get_path_prefix(self, url):
        parsed_url = urlparse(url)
        path = parsed_url.path
        if "/" in path:
            path = path[:path.rfind("/")]  # Remove filename (if any)
        return path.rstrip("/")  # Remove trailing slash for consistency

    def is_text_link(self, url):
        """
        Determines if a URL points to a text-based resource.
        """
        non_text_extensions = ['.exe', '.zip', '.png', '.jpg', '.jpeg', '.gif', '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.tar', '.gz']
        parsed_url = urlparse(url)
        if any(parsed_url.path.endswith(ext) for ext in non_text_extensions):
            return False
        return True  # Allow text-based resources like .html, .htm

    async def fetch_and_parse(self, url):
        """
        Fetch the fully rendered HTML of a URL using Pyppeteer.
        """
        try:
            rendered_html = await fetch_rendered_html(url)
            return scrapy.http.HtmlResponse(
                url=url, body=rendered_html, encoding="utf-8"
            )
        except Exception as e:
            if self.verbose:
                print(f"Error fetching rendered HTML for {url}: {e}")
            return None

    async def convert_to_pdf(self, page_url, pdf_filename):
        """
        Converts a webpage to PDF and includes the URL on the first page if `self.embed_url` is True.
        """
        browser = await launch(headless=True)
        page = await browser.newPage()
        await page.goto(page_url)

        if self.embed_url:
            # Inject URL into the first page of the PDF
            await page.evaluate(f"""
                const url = '{page_url}';
                const infoBox = document.createElement('div');
                infoBox.style.position = 'absolute';
                infoBox.style.top = '20px';
                infoBox.style.left = '20px';
                infoBox.style.fontSize = '12px';
                infoBox.style.fontFamily = 'Arial, sans-serif';
                infoBox.style.color = 'black';
                infoBox.textContent = "Source URL: " + url;
                document.body.appendChild(infoBox);
            """)

        # Save the page as a PDF
        await page.pdf(path=pdf_filename)
        await browser.close()

    def parse(self, response):
        page_url = response.url

        # Fetch and render the page if dynamic content is suspected
        rendered_response = asyncio.run(self.fetch_and_parse(page_url))
        response = rendered_response or response

        if page_url in self.visited_urls_set:  # Skip if already visited
            return
        self.visited_urls.append(page_url)  # Maintain order of visited pages
        self.visited_urls_set.add(page_url)  # Mark as visited

        if self.verbose >= 1:
            print(f"[Processed]: {page_url}")

        pdf_filename = os.path.join(self.base_dir, f"{hash(page_url)}.pdf")

        # Convert the page to PDF using asyncio directly
        asyncio.run(self.convert_to_pdf(page_url, pdf_filename))

        # Extract all `<a>` elements with an `href` attribute
        links = response.css("a::attr(href)").getall()

        # Verbose Level 3: Show extracted links
        if self.verbose >= 3:
            print(f"[Details] Extracted links from {page_url}: {links}")

        for link in links:
            # Build the absolute URL and remove fragments
            absolute_url = urljoin(response.url, link)
            absolute_url, _ = urldefrag(absolute_url)  # Strip fragment identifiers

            # Skip links that match the skip regex (if specified)
            if self.skip_regex and self.skip_regex.search(absolute_url):
                if self.verbose >= 2:
                    print(f"[Skipped] Due to regex match: {absolute_url}")
                continue

            # Check if the link is a non-text resource
            if not self.is_text_link(absolute_url):
                if self.verbose >= 2:
                    print(f"[Skipped] Non-text link: {absolute_url}")
                continue

            # Ensure the link is within the allowed domain and path prefix
            parsed_url = urlparse(absolute_url)
            if parsed_url.netloc != self.allowed_domains[0] or not parsed_url.path.startswith(self.allowed_path_prefix):
                if self.verbose >= 2:
                    print(f"[Skipped] Out-of-domain or out-of-subdirectory: {absolute_url}")
                continue

            # Follow valid links
            if absolute_url not in self.visited_urls_set:
                if self.verbose >= 2:
                    print(f"[Following]: {absolute_url}")
                yield scrapy.Request(url=absolute_url, callback=self.parse)

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def closed(self, reason):
        """
        This method is called when the spider finishes, allowing cleanup tasks to be performed.
        """
        # Save visited URLs to a file
        with open("visited_urls.txt", "w") as f:
            for url in self.visited_urls:
                f.write(url + "\n")

        # Verbose Level 1: Summarize visited URLs
        if self.verbose >= 1:
            print(f"[Summary] Total visited URLs: {len(self.visited_urls)}")

        # Verbose Level 2: List all visited URLs
        if self.verbose >= 2:
            print("[Details] Visited URLs:")
            for url in self.visited_urls:
                print(url)

        # Close any resources like the browser
        if hasattr(self, 'browser') and self.browser is not None:
            loop = asyncio.get_event_loop()
            loop.run_until_complete(self.browser.close())


def combine_pdfs(output_filename, input_dir="crawled_pdfs", visited_urls=None, verbose=0):
    """
    Combines all PDFs in the order of the visited URLs into a single output file.
    Output logging is controlled by the verbose level.
    """
    merger = PdfMerger()
    for url in visited_urls:
        pdf_filename = os.path.join(input_dir, f"{hash(url)}.pdf")
        if os.path.exists(pdf_filename):
            if verbose >= 2:  # Log added PDFs at verbose level 2 or higher
                print(f"[Added] {pdf_filename} to the combined PDF")
            merger.append(pdf_filename)
        else:
            if verbose >= 2:  # Log missing PDFs at verbose level 2 or higher
                print(f"[Missing] PDF for {url}")

    merger.write(output_filename)
    merger.close()

    if verbose >= 1:  # Confirmation at verbose level 1 or higher
        print(f"[Success] Combined PDF saved as '{output_filename}'")

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 3:
        print("Usage: python script.py <start_url> <output_filename> [--verbose=<level>] [--skip-regex <regex>] [-q] [--embed-url]")
        sys.exit(1)

    start_url = sys.argv[1]
    output_filename = sys.argv[2]

    # Default verbosity level is 1 (logs processed URLs)
    verbose = 1
    embed_url = False  # Default: Do not embed URLs

    for arg in sys.argv:
        if arg.startswith("--verbose="):
            try:
                verbose = int(arg.split("=")[1])
            except ValueError:
                print("Invalid verbosity level. Use --verbose=0, 1, 2, or 3.")
                sys.exit(1)

        if arg == "-q":
            verbose = 0  # Quiet mode, suppress all logs

        if arg == "--embed-url":
            embed_url = True  # Enable URL embedding

    skip_regex = None
    for i, arg in enumerate(sys.argv):
        if arg == "--skip-regex" and i + 1 < len(sys.argv):
            skip_regex = sys.argv[i + 1]

    process = CrawlerProcess(settings={
        "LOG_LEVEL": "ERROR"
    })

    process.crawl(PDFSpider, start_url=start_url, output_filename=output_filename, skip_regex=skip_regex, verbose=verbose, embed_url=embed_url)
    process.start()

    # Load visited URLs from the temporary file
    with open("visited_urls.txt", "r") as f:
        visited_urls = [line.strip() for line in f]

    # Call combine_pdfs and pass the verbose level
    combine_pdfs(output_filename, input_dir="crawled_pdfs", visited_urls=visited_urls, verbose=verbose)
